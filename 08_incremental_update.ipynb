{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# 08 - Incremental Update\n",
    "\n",
    "This notebook performs an incremental update of the ITA Law document database:\n",
    "1. Re-scrapes all case pages to detect new documents\n",
    "2. Updates metadata for existing documents if changed\n",
    "3. Downloads only new PDFs that don't exist locally\n",
    "\n",
    "**Prerequisites:** You should have already run notebooks 01-05 at least once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from scraper.incremental import (\n",
    "    load_existing_documents,\n",
    "    scrape_case_documents,\n",
    "    compare_documents,\n",
    "    merge_updates,\n",
    "    get_missing_pdfs,\n",
    "    run_incremental_update\n",
    ")\n",
    "from doc_download.download_docs import parallel_download_pdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-header",
   "metadata": {},
   "source": [
    "## 1. Load Existing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-existing",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_df = load_existing_documents('data/unctad_document_level_data.csv')\n",
    "print(f\"Existing documents: {len(existing_df):,}\")\n",
    "print(f\"Existing cases: {existing_df['arbitration_id'].nunique():,}\")\n",
    "print(f\"Unique doc_ids: {existing_df['doc_id'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urls-header",
   "metadata": {},
   "source": [
    "## 2. Load Case URLs to Scrape\n",
    "\n",
    "Get the unique case page URLs from the existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-urls",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique case-level data for scraping\n",
    "case_cols = [\n",
    "    'year_of_initiation', 'short_case_name', 'full_case_name',\n",
    "    'link_to_italaws_case_page', 'respondent_state', 'home_state_of_investor'\n",
    "]\n",
    "case_urls_df = existing_df[case_cols].drop_duplicates(subset=['link_to_italaws_case_page'])\n",
    "case_urls_df = case_urls_df[case_urls_df['link_to_italaws_case_page'].notna()]\n",
    "print(f\"Case URLs to scrape: {len(case_urls_df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-header",
   "metadata": {},
   "source": [
    "## 3. Test on Small Subset (Optional)\n",
    "\n",
    "Before running the full scrape, test on a small subset to verify the logic works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-subset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with first 5 cases\n",
    "test_df = case_urls_df.head(5).copy()\n",
    "print(f\"Testing with {len(test_df)} cases...\")\n",
    "\n",
    "test_cases = scrape_case_documents(test_df, delay_range=(0.5, 1.0))\n",
    "\n",
    "# Show documents found\n",
    "for case in test_cases:\n",
    "    docs = case.get('documents', [])\n",
    "    print(f\"  {case.get('short_case_name')}: {len(docs)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-compare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare against existing\n",
    "comparison = compare_documents(existing_df, test_cases)\n",
    "print(f\"New documents: {len(comparison['new'])}\")\n",
    "print(f\"Updated documents: {len(comparison['updated'])}\")\n",
    "print(f\"Unchanged documents: {len(comparison['unchanged'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "full-header",
   "metadata": {},
   "source": [
    "## 4. Full Incremental Scrape\n",
    "\n",
    "**Warning:** This will scrape all ~1,300 case pages. With polite delays, expect ~25-30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "full-scrape",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run full scrape\n",
    "# result = run_incremental_update(\n",
    "#     existing_csv='data/unctad_document_level_data.csv',\n",
    "#     case_urls_df=case_urls_df,\n",
    "#     output_csv='data/unctad_document_level_data.csv',\n",
    "#     delay_range=(0.5, 1.5),\n",
    "#     documents_dir='documents'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-header",
   "metadata": {},
   "source": [
    "### Or run step-by-step for more control:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step-scrape",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Scrape all case pages\n",
    "# scraped_cases = scrape_case_documents(case_urls_df, delay_range=(0.5, 1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step-compare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Compare documents\n",
    "# comparison = compare_documents(existing_df, scraped_cases)\n",
    "# print(f\"New documents: {len(comparison['new'])}\")\n",
    "# print(f\"Updated metadata: {len(comparison['updated'])}\")\n",
    "# print(f\"Unchanged: {len(comparison['unchanged'])}\")\n",
    "# print(f\"New cases: {len(comparison['new_cases'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step-merge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Merge updates\n",
    "# updated_df = merge_updates(existing_df, comparison)\n",
    "# print(f\"Total documents: {len(updated_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step-save",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Save updated data\n",
    "# updated_df.to_csv('data/unctad_document_level_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "download-header",
   "metadata": {},
   "source": [
    "## 5. Download New PDFs\n",
    "\n",
    "Download only PDFs that don't exist in the documents folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-missing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the updated data (or use result['missing_pdfs'] from above)\n",
    "updated_df = pd.read_csv('data/unctad_document_level_data.csv')\n",
    "missing_pdfs = get_missing_pdfs(updated_df, documents_dir='documents')\n",
    "print(f\"Documents needing download: {len(missing_pdfs):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download-pdfs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download missing PDFs\n",
    "if len(missing_pdfs) > 0:\n",
    "    print(f\"Downloading {len(missing_pdfs)} PDFs...\")\n",
    "    # results = parallel_download_pdfs(missing_pdfs)\n",
    "    \n",
    "    # Save download results\n",
    "    # with open('data/download_results_incremental.json', 'w') as f:\n",
    "    #     json.dump(results, f, indent=2)\n",
    "else:\n",
    "    print(\"No new PDFs to download.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## 6. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload and show final stats\n",
    "final_df = pd.read_csv('data/unctad_document_level_data.csv')\n",
    "print(f\"Total documents: {len(final_df):,}\")\n",
    "print(f\"Total cases: {final_df['arbitration_id'].nunique():,}\")\n",
    "print(f\"Documents with PDF links: {final_df['doc_link'].notna().sum():,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
